---
title: "Building a David Goggins Voice Chat Bot: From APIs to DIY"
publishedAt: "2024-12-02"
summary: "A journey through voice cloning - exploring commercial APIs, open-source models, and the challenges of building a Goggins-voiced chat bot."
tags: "Voice Cloning, AI, TTS, Side Project"
---

## The Idea

<Caption>
  Imagine being able to have a pep talk with David Goggins whenever you need one, getting that raw motivation and energy exactly when you need it most.
</Caption>

The idea came to me while watching one of Goggins' motivational videos - what if I could build a chat bot that could capture not just his words, but his entire essence? A system where you could speak naturally, and it would respond with Goggins' characteristic intensity and wisdom, complete with his distinctive voice and delivery style.

## The Architecture

The high-level flow I envisioned was more complex than it initially seemed. The system needed to handle multiple transformations seamlessly: converting spoken words to text, generating contextually appropriate responses in Goggins' style, and then converting those responses back into speech that genuinely sounded like him. Here's what the complete flow looked like:

1. User speaks to the bot (voice input)
2. Speech-to-Text (STT) service converts the audio to text
3. GPT-4 generates a response matching Goggins' personality and communication style
4. Text-to-Speech (TTS) system converts the response to audio in Goggins' voice
5. The response is streamed back to the user in real-time

## Commercial APIs: The First Attempt

The journey began with exploring commercial voice cloning services, including ElevenLabs, Speechify, and PlayHT. While most services had restrictive free tiers, PlayHT has 12,500 characters generation in their free tier - enough to experiment with. 
After uploading a short reference clip from one of Goggins' YouTube videos, the results were surprisingly impressive. Check them here:

<Video
src="/videos/playht-goggins-sample.mp4"
controls
style={{
maxWidth: "100%",
height: "auto",
borderRadius: "8px",
marginTop: "1rem",
marginBottom: "1rem"
}}
/>


This introduced me to [Zero-shot voice cloning](https://play.ht/blog/zero-shot-voice-cloning/), 
where it analyze and replicate a voice's characteristics from just a few seconds of reference audio, 
without requiring extensive training data or fine-tuning.

## Building with PlayHT API

PlayHT's documentation suggested [LLM streaming support](https://docs.play.ht/docs/input-and-output-streaming-with-chatgpt-or-any-llm), which seemed perfect for creating a fluid conversation experience. I integrated their API with Claude and GPT-4 to build a proof of concept, 
but the streaming functionality didn't work as documented, forcing me to implement a little crappy solution where I had to record the entire response, send it for processing, and then play it back. Check the demo here:

<Video 
  src="/videos/voice-chat-goggins-demo.mp4" 
  controls 
  style={{ 
    maxWidth: "100%", 
    height: "auto", 
    border: "1px solid #ccc", 
    borderRadius: "8px" 
  }} 
/>

## Exploring Open Source Models: Coqui TTS

As I approached the limits of PlayHT's free tier, I discovered [Coqui TTS](https://docs.coqui.ai/en/latest/), an open-source text-to-speech framework. The prospect of having unlimited generations without API costs was exciting, leading to an ambitious plan:

1. Train the model using Goggins' audio content
2. Host the solution myself
3. Create an unlimited personal voice bot
4. Fine-tune the voice to match Goggins' energy

I was entering into a complex, time taking task for a nervous beginner buit I did it any way, because:

<div className="flex justify-center">
  <Image 
    src="/images/what-else-we-here-for.jpg"
    alt="What else we here for meme"
    width={300}
    height={300}
    className="rounded-lg my-4"
  />
</div>

## Technical Implementation and Optimizations

The initial implementation led to a significant challenge - inference latency. 
The time taken to convert text to audio in Goggins' voice was averaging 20-30 seconds depending on the text length, making real-time conversation impossible. 
This led to a series of optimizations because I don't have a GPU:
1. **DeepSpeed Integration:**
   - Implemented DeepSpeed for faster model inference
   - Configured optimal parameters for batch processing
   - Enabled FP16 precision to reduce memory usage

2. **Text Processing:**
   - Implemented intelligent text chunking
   - Optimized chunk sizes for better processing
   - Added parallel processing for multiple chunks

3. **Voice Pattern Storage:**
   - Implemented speech embedding storage
   - Created a caching system for frequently used phrases
   - Stored voice patterns to avoid repeated cloning

4. **Parameter Tuning:**
   - Experimented with different temperature settings
   - Adjusted length penalty for optimal output
   - Fine-tuned the model's hyperparameters

Despite these optimizations, the inference latency remained a bottleneck - we managed to reduce it to about 15-20 seconds, but this was still far from the near-real-time experience I was aiming for.

## The GPU Quest and Hosting Ambitions

The search for better performance led me to explore various cloud GPU options:

1. **Google Colab:**
   - Provided some improvement in latency (down to 10-12 seconds)
   - Free tier limitations made consistent usage impossible
   - Frequent disconnections disrupted development

2. **Hugging Face:**
   - Promised free GPU-powered hosting
   - Turned into a complex dependency resolution challenge
   - Struggled with numpy version conflicts
   - Resulted in an astonishing 19-minute response time

After setting up all of this and tons of to and fro with Claude I realised the real costs of "free" services - while they might not charge money, they extract their price in time, effort, and frustration.

## The Final Demo

The local implementation, while not perfect, demonstrated the potential of the concept. Have some patience while watching:

<Video 
  src="/videos/Local-tts-model-demo.mp4" 
  controls 
  style={{ 
    maxWidth: "100%", 
    height: "auto", 
    border: "1px solid #ccc", 
    borderRadius: "8px" 
  }} 
/>

The audio quality was decent I feel, capturing Goggins' unique vocal characteristics, 
but the latency made it impractical for the kind of dynamic, motivational conversations I had throught of.

## Key Learnings

This project gave a lot of new info about:
- The complexities of deploying machine learning models
- The critical role of inference optimization
- The balance between audio quality and processing speed
- The importance of GPU resources in ML applications
- The real-world challenges of building AI-powered applications

## Future Possibilities

<Callout emoji="ðŸš€">
As GPU technology advances and becomes more accessible, the dream of having personal AI-powered conversations with our favorite personalities comes closer to reality. Just as we stream Kanye's music on-demand today, tomorrow we might be having real-time conversations with AI versions of inspiring figures like Goggins.
</Callout>

While current technology limitations prevented the creation of the seamless experience I envisioned, the project demonstrated both the potential and challenges of voice cloning technology. The future holds exciting possibilities as hardware becomes more powerful and accessible, potentially making low-latency voice chat with AI personalities a reality.

Stay hard! ðŸ’ª